<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" class="no-js no-jr">
<head>
   <title>Article 2</title>
   <meta charset="utf-8">

   <!-- custom stuff for sils-annotator -->
   <link rel="stylesheet" href="{{ url_for('static', filename='css/annotator.css') }}" />
   <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}" />

   <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
   <!--<script type="text/javascript" src="{{ url_for('static', filename='js/jquery-1.9.1.js') }}"></script>-->
   <script type="text/javascript" src="{{ url_for('static', filename='js/underscore.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/jquery.scrollTo-1.4.3.1-min.js') }}"></script>

   <script type="text/javascript"> apiRoot = "{{ g.api_root }}"</script>


   <!-- stuff for the off-the-shelf annotator.js -->

   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/preamble.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/class.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/console.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/extensions.js') }}"></script>


   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/range.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/annotator.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/plugin/store.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/plugin/scrollbar.js') }}"></script>


   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/widget.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/editor.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/viewer.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/notification.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/silsannotate.js') }}"></script>

</head>
<body>
   <h1>Usable guidelines for usable websites? An analysis of five e-government heuristics</h1>
   <p>Marieke Welle Donker-Kuijera, Menno de Jonga, Leo Lentz</p>

   <h2>Abstract</h2>
   <p>Many government organizations use web heuristics for the quality assurance of their websites. Heuristics may be used by web designers to guide the decisions about a website in development, or by web evaluators to optimize or assess the quality of an existing website. Despite their popularity, very little is known about the usefulness of heuristics for web designers and evaluators. In this study, five government heuristics are examined with special attention to their presumed use, content, validity, and presentation format. Our findings raise questions about the usability of heuristics for web design and web evaluation purposes. More research into the actual use and effects of web heuristics is needed.</p>

   <h3>Keywords</h3>
   <p>Websites; Formative evaluation; Expert evaluation; Usability; Heuristics; Guidelines</p>

   <h2>1. Introduction</h2>
   <p>All over the world, government organizations try to improve their service to citizens and companies using the possibilities offered by ICT. <a href="#29">West's (2008)</a> annual analysis of e-Government websites worldwide illustrates their growing popularity. West found that 96 percent of 1,667 e-Government websites from 198 nations provide access to publications and 75 percent have links to databases. In addition, 50 percent of the websites offer services that are fully executable online. This percentage is up from 28 percent the year before. Government organizations see websites as a promising means to increase the involvement of citizens and to improve their efficiency <a href="#13">(Mahler &amp; Regan, 2007)</a>. However, it is questionable whether all websites fulfill the promises that they make. Several studies show that the actual use of the information and services offered on these websites is still limited (<a href="#9">Hart and Teeter, 2003</a>, <a href="#10">Holden et al, 2003</a>, <a href="#25">Van Deursen et al., 2006</a> and <a href="#26">Van Dijk et al., 2007</a>). Usability problems are among the reasons for the underuse of e-Government websites (<a href="#10">Holden et al., 2003</a> and <a href="#26">Van Dijk et al., 2007</a>). Government organizations therefore need to critically evaluate the content and usability of their websites (<a href="#3">Bertot &amp; Jaeger, 2008</a>).</p>

   <p>Web designers need evaluation methods to identify possible problems with their websites. They have many evaluation methods from which to choose, originating from fields such as human-computer interaction and document design. A distinction can be made between expert-focused and user-focused methods (<a href="#18">Schriver, 1989</a>). Both have clear and useful functions in design and evaluation processes and neither one can replace the other (<a href="#14">Mayhew, 2005</a>).</p>

   <p>A very popular expert-focused evaluation method is heuristic evaluation, an approach developed by Nielsen and Molich in the early 1990 s (<a href="#15">Nielsen, 1994</a>). In a heuristic evaluation, one or more experts check the website using a predefined set of evaluation criteria, the heuristics. Central to a heuristic evaluation is the set of heuristics used. The heuristics developed by Nielsen and Molich are widely published on the internet and in books and articles. They were originally developed for the evaluation of user interfaces, and consist of ten items, derived from a factor analysis of problems found in usability studies with user interfaces. Nowadays, this set of heuristics is applied to a wide variety of ICT applications, including websites. Because they have their origins in human factors, these heuristics have a strong usability perspective.</p>

   <p>Following the description of <a href="#5">De Jong and Van der Geest (2000)</a>, we see heuristics as “all the sets of process guides, principles, criteria, tips and tricks, and guidelines that are available to support Web designers” (p. 311). For the purposes of our study, heuristics are predefined sets of principles website designers and evaluators can use “to identify problems, to get an overview of the range of options to choose from or to help assess the qualities of an option” (<a href="#5">De Jong &amp; Van der Geest, 2000</a>, p. 311). Heuristics may pertain to very different aspects of websites, such as the navigation, lay-out and color use, but also to usability, accessibility, and information quality.</p>

   <p>Heuristic evaluation is attractive for practitioners, because it offers several advantages. One advantage is that it provides quick, and relatively inexpensive, feedback, and the results usually generate good ideas for improving a website. The fact that the method can be used in both the design and the evaluation phase, is also advantageous, because experts can apply the same criteria to different phases of the design process, which creates some degree of “common ground” within the process. This is particularly useful when experts from different backgrounds work on a website, just as using the same set of criteria creates a shared frame of reference for discussing website quality (<a href="#5">Van der Geest &amps; Spyridakis, 2000</a>). To take this point one step further, heuristics are predefined sets of criteria, which implies the existence of general quality criteria to which all sites should adhere. The existence of such general criteria creates benchmark opportunities between websites.</p>

   <p>Although heuristic evaluation is very popular, little is known about the way heuristics function. Several issues are at stake here. The first issue regarding the functionality of heuristics involves the role of the evaluator. Using heuristics to evaluate a website implies an interaction between the evaluator's expertise and the directions included in the heuristics (<a href="#28">Welle Donker-Kuijer, De Jong, & Lentz, 2008</a>). The way experts handle this job is underexposed in the literature. The second issue relates to the content of the heuristics and the way they represent state-of-the-art knowledge about effective web communication. The existence of heuristics suggests that their usage will contribute to successful websites, because they supposedly contain the most relevant and current advice on web design. However, it is uncertain whether this claim is always justified. Only a few studies have investigated whether websites redesigned according to heuristics are indeed more effective, and the few studies that are available “only” show improvements within the parameters of the subject of the heuristics (<a href="12">Leporini and Paternò, 2008</a>, Tang et al., 2006 and <a href="#20">Uldall-Espersen et al., 2008</a>).</p>

   <p>This study is a first attempt to shed more light on these issues. We will examine heuristics provided by governmental organizations. Many (inter)national government bodies provide heuristics for e-Government websites. We analyzed five e-Government heuristics and explored their underlying assumptions. First, we will describe the overall appearance of these five heuristics to get a feel for these documents. After that, we will focus more specifically on four aspects of e-Government heuristics: (a) their context of use, (b) the information they cover, (c) their validity , and (d) their presentation format.</p>

   <h2>2.Method</h2>
   <p>This section will review the selection of the particular heuristics that we included in our analysis. After that, we will explain how we analyzed the overall appearance of the heuristics, and how we used the framework by <a href="#5">De Jong and Van der Geest (2000)</a> for a more detailed analysis. The section concludes with an explanation of the procedure used for the study.</p>

   <h3>2.1. Selection of heuristics</h3>
   <p>For our analysis of e-Government heuristics we selected five heuristics: four from national governments and one from a “transnational” government. The heuristics are made available on e-Government websites, which all function as “advisory boards” for the promotion of e-Government by the respective governments. The five heuristics selected for the study are as follows:</p>

   <p>The “Guidelines for UK Government Websites” are published by the e-Government Unit of the Cabinet Office of the UK government (<a href="#16">Office of the e-Envoy, 2003</a>).</p>

   <p>The “Overheid.nl Web Richtlijnen versie 1.2” (Government.nl Web Guidelines version 1.2) are published online by AdviesOverheid.nl, a Dutch government agency founded to support government bodies with their websites (<a href="#1">Advies Overheid.nl, 2005</a>).</p>

   <p>The EU Information Providers Guide presents the “10 Top Ten Guidelines” that all European Union websites must adhere to and is made available on the website of the European Union (<a href="#6">European Commission</a>).</p>

   <p>The “Barrierefreie Informationstechnik-Verordnung Checkliste” (BITV, Barrier Free ICT Act) is published by German government agencies (<a href="#4">Bundesamt für Sicherheit in der Informationstechnik (BSI), 2005</a>).</p>

   <p>The “Section 508” standards are published by the <a href="#22">USA Architectural and Transportation Barriers Compliance Board or Access Board (2000)</a>.</p>

   <h3>2.2. Analysis of the overall appearance of the heuristics</h3>
   <p>Aspects of the overall appearance in which we were interested were the way the heuristics are made available, their composition, their complexity and the topic of the heuristics.</p>

   <p>Heuristics can be made available to the public via different media, such as books, journals, popular magazines, and the internet. Each of these forms may affect the way the heuristics can be used. Issues that could arise include price, printing possibilities, and sharing the document with other evaluators. Practitioners may have trouble accessing heuristics published only in print because of limited availability in stores and libraries or the high cost of the copy. Sharing information in print, especially in the shape of a book, amongst members of an evaluation team may also be more difficult than sharing (originally) digitally available information. All of the heuristics included in this analysis were found via the internet. In addition, we checked the websites for other means of publication.</p>

   <p>The composition of a set of heuristics can differ significantly. On one side of the spectrum we can find long stretches of running text, whereas on the other side of the spectrum other heuristics are structured as short checklists. In between these two extremes, we find texts with some form of structural markers such as enumerations and indentations. Longer documents can be made more usable by adding tables of contents, indexes, glossaries, and advance organizers. All of these elements can help the user find relevant information more easily. With heuristics written as running text, it may be difficult to gather the key points from the text, but with heuristics shaped as a short checklist there may not be enough space for supporting argumentation or clarifying examples. For this study, the composition was described by an overall inspection of the document. We looked for running text, different structural markers and checklists, tables of contents, indexes, glossaries, advance organizers etc.</p>

   <p>The complexity of the document is also a crucial issue. When choosing heuristics to use, experts immediately have to assess how big the document is, which parts to actually use, and how easy it is to find information within the document. This aspect is related to the composition of the document. A book with a comprehensive index and checklists summarizing the key points of each chapter is likely to be perceived as less complicated than an unstructured set of web pages containing such information. Decisions about the complexity of the document were based on combinations of these factors: the size, the ratio between the “usable” and “unusable” parts, and the ease of finding evaluation criteria within the heuristics.</p>

   <p>Differences in the main topics of the heuristics are likely to lead to very different problem detections. A set of heuristics focusing on accessibility point the expert in a very different direction than do a set of heuristics focusing on, say, information quality. <a href="#7">Paddison and Englefield (2004)</a> have shown that experience in applying one type of heuristics does not necessarily mean an expert can easily perform an evaluation with another type of heuristics. We based our decision about the topic of the heuristics on the content of the items.</p>

   <h3>2.3. Analysis based on the framework for categorizing heuristics</h3>
   <p>We examined four aspects of the heuristics, based on a framework for characterizing heuristics developed by <a href="5">De Jong and Van der Geest (2000)</a>: context of use, information covered by the heuristics, validity, and presentation format.</p>

   <h4>2.3.1. Context of use</h4>
   <p>The way heuristics can be used was analyzed by looking at the focus of support, the function in the design process, and the assumptions made about actual use of the heuristics. The focus of support of heuristics can be on the website itself, the process of producing and managing the website, or the (organizational) context in which the website must function (e.g., responsibilities, operating systems, or intended audiences). We classified all heuristic items accordingly.</p>

   <p>Heuristics can have three functions in the design process: troubleshooting, verifying, and idea-generating. The most common goal of heuristic evaluation is troubleshooting; practitioners evaluate a website to detect problems. Heuristics can also suggest solutions for these problems and give web designers ideas to improve the website. Troubleshooting and idea-generating can thus be seen as two sides of the same coin. The verifying function of heuristics is fundamentally different from the other two functions. If a website complies with certain heuristics, this can lead to a sort of official approval. Examples of verifying heuristics are the WCAG 1.0 accessibility guidelines (<a href="#27"> W3C, 1999</a>). The label “verifying” only applied if correct application of the heuristics resulted in an “official seal of approval.”</p>

   <p>Finally, based on the size and structure of the heuristics, we focused on the assumed use of the heuristics. Given their size and complexity, some heuristics are clearly meant for external representation (e.g., in the form of checklists). Others are smaller or very well structured, so that they can be memorized and internalized by the evaluator. Our assumptions about the actual use of the heuristics were based on the size and structure of the heuristics. Heuristics that were very large, or where the structure was unclear, were labeled as external representation. Others that were smaller or very well structured, so that they could be expected to become internalized by the evaluator, were classified as internal representation.</p>

   <h4>2.3.2. Information covered by the heuristics</h4>
   <p>We made a distinction between two types of issues in website guidelines: focus of the content and relation to technology. The focus of the content involved the distinction in guidelines focusing on accessibility, usability, programming language, and other characteristics. Examples of technology-related issues are hardware and software requirements, programming languages, printing options, filenames, alt-text for images, and moving animations. All items that referred to issues that do not require the use of technology and that, for instance, could also apply to paper documents were coded as not technology-related.</p>

   <h3>2.3.3. Validity of the heuristics</h3>
   <p>Relevant issues with regard to the validity of the heuristics include the foundations upon which the heuristics are built and the availability of validation research. Validation research – the assessment of the effectiveness of websites after applying the heuristics – is not available for these heuristics. Our analysis therefore focused on the heuristics’ foundations. Are the heuristics based on research, theory, standards or practitioner experience? Each of these foundations has its own merits, but web designers may have more confidence in research-based heuristics than in practitioner-based ones. On the other hand, <a href="#7">Evans (2000)</a> showed that the process of developing research findings into a set of heuristics is not a walk in the park. The analysis of the foundations of heuristics was based on the accompanying information for the heuristics.</p>

   <h4>2.3.4. Presentation format of the heuristics</h4>
   <p>Four aspects of the presentation format were analyzed: structure, formulation of items, openness of questions, and level of items. The structure is important, because a meaningful structure may help experts to maintain an overall perspective on the heuristics and to oversee the intention of the heuristics. Besides, a meaningful structure may help experts to memorize (parts of) the heuristics. If the heuristics were structured in any way, e.g. in chapters, we scored this as “meaningful structure.”</p>

   <p>Items can be formulated as instructions, questions, or requirements. The effect of the formulation of items on the experts is still unclear, but might correspond to different activities in the design process (e.g., designing a new web page versus evaluating an existing page). Whether items were scored as instructions, questions, or requirements was determined by their sentence structure. If items were considered a question, we also checked whether it was an open or closed question. Open questions may help to generate more ideas, whereas closed questions might structure and curtail the process.</p>

   <p>Publishers of heuristics have to decide whether to limit themselves to a relatively small set of general guidelines or to give a larger set of more concrete and detailed guidelines, possibly complemented with extra examples. These high-level and low-level items both have their advantages and disadvantages. High-level items might be open to different interpretations, which could easily lead to disagreements in problem detections between different evaluators. On the other hand, heuristics consisting of low-level items are likely to become very bulky documents in which a practitioner may find it difficult to keep track of all the different elements. Also, it is unclear whether an expert can extrapolate the corresponding high-level abstract heuristic from a number of low-level items. High-level heuristics generally focus on the goal that needs to be achieved, whereas low-level heuristics generally focus on the action that needs to be performed ( <a href="#8">Hale &amp; Swuste, 1998</a>). The level of items was assessed by considering whether the item referred to a goal or an action and the “openness” of the item, i.e. the amount of speculation possible.</p>

   <h3>2.4. Procedure</h3>

   <p>The analyses were performed on the heuristics as a whole and on the various items included in them. The categories overall appearance, foundations, structure, function in the design process, and assumptions about actual use were coded on the level of the heuristics as a whole, whereas the categories focus of support, relation to technology, focus of content, formulation of items, openness of question, and level of items were coded on the level of individual items.</p>

   <p>The heuristics were collected in one database, dividing all heuristics in different items. Dividing the heuristics in separate items proved to be much harder than initially expected. For example, at first sight, the British Top 10 guidelines in the executive summary consisted of only 10 guidelines. Closer inspection, however, showed that to each guideline 5 to 18 articles were added, some containing a justification for the guideline, and others containing practical advice. The same problem occurred in the Information Providers Guide from the European Union. Quite often the “higher-level” guideline contained one or more checklists detailing actions to achieve the goal of the guideline. These checklists were usually very focused on technical aspects, for example giving specific HTML code to achieve certain results. The German BITV checklist consisted of 14 requirements that were followed by 66 preconditions. We solved the problem of these “nested” items by taking the “higher-level” guideline as the unit of analysis. The authors of these guides present these as the guideline and we follow their interpretation.</p>

   <p>In all, we included 581 items in the analysis. The Guidelines for UK Government Websites contained 348 items (we included the top ten guidelines mentioned in the executive summary and all checklists). The Section 508 heuristics contained 25 items (subpart B, C and D divided in 16, 6 and 3 items per section, from subpart B we only analyzed the guidelines regarding web-based intranet and internet information and applications) and the German BITV-checklist contained 14 items. The Dutch Web Guidelines contained 184 items, divided in 30 (requirements), 30 (tests) and 124 (guidelines) items per section. The EU Information Providers Guide contained 10 items.</p>

   <p>To check inter-rater reliability for focus of support, relation to technology, focus of content, and level of item, the first two authors independently coded 25 percent of the items. Inter-rater reliability was calculated using Cohen's kappa, which ranged from .61 for focus of support to .68 for technology-relatedness, indicating substantial agreement between the raters ( <a href="#11">Landis &amp; Koch, 1977</a>). The first author then coded the other 75 percent of the items.</p>

   <h2>3. Overall appearance of the five sets of e-Government heuristics</h2>

   <p>What is the look and feel of the five sets of e-Government heuristics? We will present our analysis of the overall appearance of the separate heuristics and then summarize the findings.</p>

   <h3>3.1. Guidelines for UK Government Websites</h3>
   <p>At first sight the Guidelines for UK Government Websites seem a fairly convenient set of heuristics with which to work. The full document can be obtained from the website as HTML, PDF, and MS Word files, but is also available as a book. The 46 different subsections of the book can be downloaded and printed as separate PDF documents, but there is no PDF of the whole book. The digital availability of the book makes it easy to share within a team, but downloading the 46 subsections takes a bit of time and effort. This set of guidelines is very detailed, with advice and explanations about all conceivable topics (see <a>Table 1</a>). Every guideline can be found easily using the table of contents and the index included in the annexes. The “top ten guidelines” are summarized in the management summary and four important topics (Auditing and Statistical Analysis, Choosing an ISP/Hosting Service, Universal Usability, and Specifying Your Website) are covered in extensive checklists. In addition, every section starts with a short checklist and summary of its contents. The rest of each section consists of running text combined with a division into subsections, enumerations, and information boxes. The document also contains a 26-page glossary.</p>

   <p>However, there is some friction with these heuristics. At first glance, one finds many useful tips and tricks and everything seems easy enough to find. A web designer who needs to know all of the requirements to which the website needs to adhere, will nevertheless have more difficulty. So much information is included that it is difficult to separate the main from the side issues. These heuristics seem to suffer from “overcompleteness.” The users of these heuristics will find much useful information, but there is so much that it is easy to get lost in the heuristics and lose track of the key points. It is also confusing that the section about accessibility contains two checklists, at the section beginning and at the end, while both checklists seem to contradict each other on (minor) points.</p>

   <h2>3.2. Overheid.nl Web Guidelines</h2>
   <p>Similar to the British guidelines, the Dutch Overheid.nl Web Guidelines version 1.2 are a fairly comprehensive set of heuristics in which many topics are covered. The most striking feature of these heuristics is the division into different functions. The Overheid.nl Web Guidelines consist of requirements, tests, and guidelines that are interlinked. Each requirement has a matching test. For example, the requirement E-1.2 “Information on the websites will be available for all people and machines” is matched by the test T-1.2 “Is the information on the websites available for all people and machines?” The Web Guidelines lists 30 requirements and 30 tests. The requirements and tests can be met by following the guidelines, the third group of items. For example, the requirement and test mentioned above can be met by following the guidelines on descriptive mark-up. There are considerably more guidelines than requirements and tests.</p>

   <p>The heuristics are available as an interlinking website and as a PDF document. The latter makes downloading, sharing and printing easier. The PDF document contains the requirements, tests and guidelines and offers additional information on the way the heuristics are developed and a manual detailing how to do everything. This document includes an extensive table of contents and a glossary. The document contains internal links and external links to other related information on the internet, e.g., the W3C website. On the website, one can find a clickable overview of all the guidelines, with links to relevant subsections of the manual.</p>

   <p>The requirements and tests are clustered in three groups (Content &amp; Information, Functionality &amp; Usage, and Management). The 124 guidelines are clustered in 18 groups, corresponding to the different chapters in the manual (cf. <a>Table 2</a>).</p>

   <p>Just like the British guidelines, this is a fairly broad set of heuristics, which offer the expert information on a multitude of topics. It is a very complex document, both despite and due to the division in requirements, tests, and guidelines. The division between requirements and tests is useful, because it recognizes that different formulations are suitable for different use contexts. In this sense, it adds to the usability of the heuristic. However, the division also adds complexity to the guidelines, and more clicking to and fro to find the appropriate information. Because of the availability on the web, it is fairly easy to share the information within a team, but (shared) offline use is relatively difficult given the size of the PDF document. As opposed to the British guidelines, the Dutch guidelines only offer the opportunity to print the whole 283-page document.</p>

   <h2>3.3. EU Information Providers Guide</h2>
   <p>The EU Information Providers Guide is available both as an interlinked set of 11 HTML pages and as a 79-page PDF document labeled as “download” on the website. The HTML pages start with an opening page listing the ten guidelines, which all link to subpages containing more information about the respective guidelines (cf. <a>Table 3</a>).</p>

   <p>In addition, the opening page offers links to sections with tips and tricks, a library containing templates and standard messages, an index and sitemap, a download option, and a page listing all updates for the site. Although the website states that the EU Information Providers Guide presents ten guidelines, many more checklists are included in the subpages of this document, e.g. checklists for accessibility and quality. This makes the Guide a more complex document than initially presumed. However, other features of the composition mitigate this problem. The opening page functions as a table of contents and offers access to an index. To further facilitate finding the relevant information, each subpage has a set structure, listing the heuristic, a justification, a description and procedures and/or checklists for accomplishing the goal. The page listing updates makes it easier for repeat visitors to see whether information they use has been changed. If the intent of these heuristics would be to strive to follow the latest developments in web design, this page would be a practical means for doing so.</p>

   <p>As with the British and Dutch heuristics, the availability on the web and printing possibilities facilitate the sharing of information within a team. However, the printing possibilities could be improved by offering a PDF download per item instead of the whole document.</p>

   <h3>3.4. Barrierefreie Informationstechnik-Verordnung Checkliste</h3>
   <p>The German BITV Checklist can be found via the websites of two government agencies. The Federal Agency for Information Technology publishes the text of the checklist on an HTML page with a link on the page to a PDF document. The checklist can be found in a section of this agency's website that deals with accessibility, six clicks away from the homepage. The other way to find the checklist is via the website of a second government agency, the Bundesamt für Sicherheit in der Informationstechnik (Federal Office for Information Security). One of the modules deals with accessibility; it is a 140-page PDF document with background information about the necessity of accessible internet, (international) laws and guidelines in this field, instructions for the design of accessible websites, communication (options) and testing possibilities. The BITV checklist itself is published in the annex of this document.</p>

   <p>The document offers users much information about all matters relating to accessibility, thereby giving them a firm frame of reference by which to understand and use the checklist. The main document is composed of running text, whereas the checklist is designed as a table. The checklist consists of 14 requirements that need to be followed. These are supplemented by 66 preconditions, which can be seen more as detailed guidelines. The priority 1 preconditions (46 guidelines) should be implemented on all new or significantly changed websites, whereas the priority 2 preconditions (20 guidelines) need to be followed on all central government webportals. The annex is divided into two checklists, the first with the requirements and the priority 1 preconditions, the second with the requirements and the priority 2 preconditions. This division adds complexity. However, the relatively short list of requirements combined with the extensive background information in the main document seems fairly usable for practitioners.</p>

   <h3>3.5. Section 508</h3>
   <p>The Section 508 heuristics can be found via the website of the USA Access Board. This agency publishes the standards with explanations on HTML pages and in a 30-page PDF document located in the Federal Register. Originally, the Section 508 standards were published in the Federal Register of December 21, 2000. They are also incorporated in the Federal Acquisition Regulation. It consists of a 23-page preamble and six pages presenting the standards themselves. The preamble covers the history behind the standards, “regulatory process matters” and an extensive discussion of the standards and comments on the draft version of the standards.</p>

   <p>The whole document consists mainly of running text, but the standards themselves consist of four subparts, labeled (a) General, (b) Technical Standards, (c) Functional Performance Criteria and (d) Information, Documentation, and Support. In these subparts, the standards themselves are listed in an itemized fashion. Because of the limited scope of these heuristics, practitioners will probably have little difficulty using them. The availability on the web and the modest size of the PDF document make printing and sharing fairly easy.</p>

   <h3>3.6. General summary of the five heuristics</h3>


   <p>All the heuristics in this analysis are made available via websites using formats such as HTML, PDF and MS Word. This online availability makes it easy for practitioners to get hold of the heuristics and share them within a team. To facilitate offline use, the websites should offer the possibility to print both the whole heuristics and separate sections.</p>

   <p>On most websites a range of background information regarding the heuristics is presented, either within the set of heuristics or in additional documents available through the website. Different functions seem to be embedded within the heuristics; the heuristics themselves, the justification for the heuristics as a whole and for items within the heuristics, instructions or explanations for the heuristics and the items contained within them, and the obligation for web developers to use the heuristics. On the one hand, the heuristics might be difficult to interpret or apply without all the background information, but on the other hand, practitioners may have difficulty separating the main from the side-issues if documents become too large. A second threat to a clear overview of the heuristics is the tendency to include nested checklists. Examples of this can be found in the German and EU heuristics.</p>

   <p>We found different formats. A blend of chapters of running text and short checklists that can act as advance organizers or summaries seems to be the most valuable combination for the users of the heuristics. Inexperienced users benefit from this format because it offers them extra information and guidance, while long-term users can easily get to the core of the information.</p>

   <p>An important aspect of all heuristics is a strong focus on accessibility. Given the public character of e-Government websites, universal accessibility is a logical goal. All five heuristics refer to the WCAG 1.0 heuristics. In the preamble of the Section 508 standards, there is an explicit reference to the WCAG heuristics. A comparison of the two reveals that the main difference is that Section 508 is more limited in scope—e.g., the amount of support for all types of disabilities is more limited than in WCAG 1.0 (<a href="#1">Anderson, Bohman, Burmeister, &amp; Sampson-Wild, 2004</a>). This limitation makes it easier to codify requirements into law and verify compliance with Section 508.</p>

   <p>In addition to this strong focus on accessibility, the heuristics published by the EU, the British and Dutch government also give advice on other aspects such as the management of the website and various technical aspects. One can conclude that the US and German heuristics are most limited in scope, whereas the British and Dutch heuristics have a very broad focus. The EU heuristics fall in between. A strong focus on one topic has the added advantage of restricting the size of the heuristics and thereby reducing their complexity. However, the British and Dutch heuristics offer valuable advice on many other relevant topics. Users of the other heuristics presumably will have to find this information via other means.</p>

   <h2>4. Detailed characteristics of the heuristics</h2>
   <p>In this section we will describe the more detailed analyses of the five heuristics, using the aforementioned framework by <a href="#1">De Jong and Van der Geest (2000)</a>. We will discuss the context of use, the content, the validity, and the presentation format of the five heuristics in separate subsections.</p>

   <h3>4.1. Context of use</h3>
   <p>All items were first analyzed for their focus of support (see <a>Table 4</a>). Over the whole set, 75 percent of the items were concerned with the product itself, the website, whereas the other 25 percent of the items were almost equally divided in context- and process-focused items. However, this division over the categories differed between the heuristics. The German, USA, and Dutch heuristics almost exclusively provided guidelines about the website itself (product-focused items), while the British and EU heuristics contained much more advice about the process and context surrounding the website.</p>

   <p>The context-items dealt with four types of considerations: 1) the function of the website within the organization, 2) organizational concerns, 3) technical concerns and 4) concerns involving audience. Some examples of context-items that involve these considerations include:</p>

   <p>“The Commission can disseminate information on the Internet only through the EUROPA site under the following address europa.eu.int/comm” (EU), and</p>
   <p>“What type of server/operating system are you getting? (Mac, UNIX, Linux)? Performance, eg, how fast? Do you want MS FrontPage extension support or to use an ASP database?” (UK).</p>

   <p>The process-oriented items made up about 20 percent of the items in the British and EU heuristics. Some example of process-oriented items included:</p>

   <p>“The website needs to be manageable/exploitable in an effective and efficient manner” (NL), and</p>

   <p>“Sites and information disseminated on EUROPA should be subject to regular quality controls” (EU).</p>

   <p>The advice contained in the context- and process-items seems to indicate these heuristics are also intended as reference guides for their users on how to organize everything around the website. This advice can be very useful for web managers, but the question is how compliance with these guidelines can be ensured. Do the authors of these heuristics expect their users to follow these context- and process-items to the letter, or are they merely seen as useful extras? In the former instance, compliance is difficult to check, much more so than with product-focused heuristics. This point is related to the next issue, the function in the design process.</p>

   <p>If we look at the function in the design process, all heuristics are, first and foremost, intended for troubleshooting and idea-generating. The product-oriented items may help the web designer diagnose and solve potential problems in the website. In addition, these heuristics had a third function in the design process, the verifying function. This means that by using these heuristics, web designers can certify that their website meets the legal requirements. All the heuristics contained paragraphs implying that the use of the heuristics is obligatory or even a legal requirement. The executive summary of the UK handbook states that “it is expected (…) that all departments and agencies developing government websites will make all reasonable efforts to comply with them”. According to its introduction, the Information Providers Guide is intended for all authors of pages on EUROPA, the website of the EU, and its use is compulsory. The USA, Dutch, and German governments have taken stronger measures. The Dutch parliament has ruled that all newly built Dutch government websites must comply with the guidelines, and pre-existing websites must comply before 2010. In addition, Advies Overheid.nl publishes a monthly ranking of all government websites, which is partly based on the requirements in the Dutch Web Guidelines. The BITV checklist specifies how all new German e-Government websites need to be accessible, which is required by law since 2002. Existing websites were required to become accessible before the end of 2005. Section 508 is legally binding for all US Federal agencies and some states have also chosen to adopt this checklist of accessibility requirements as their standard for Web accessibility. This makes Section 508 very influential. If we relate the verifying function of these heuristics to the process- and context-focused items described above, one has to wonder about the possibilities of checking compliance with the heuristics. Compliance with product-focused heuristics can be verified externally by evaluating the website itself, either automatically or manually, but doing this for the other two types of heuristics requires much more effort and contact with the organization behind the website.</p>

   <p>Regarding the assumptions about actual use, the five government heuristics seemed to be designed for external representation, which means that designers and evaluators need (part of) these heuristics at all times during the various stages in the evaluation process. The EU Information Providers Guide consists of only ten items, but several of these contain a large number of items in embedded checklists. This makes it less likely that all of the heuristics will be memorized. Going back and forth between the website, the heuristics, and possibly other reference materials, might be quite a challenge. The usability of these heuristics for web designers and evaluators has probably not been a central issue.</p>

   <h3>4.2. Content of the heuristics</h3>
   <p>The first aspect of content that was analyzed was the relation to technology. As could be expected, the majority of the heuristic items were technology-related (see <a>Table 5</a>). These heuristics concern requirements or instructions that can only be met by the application of technology. Examples range from “Use Cascading Style Sheets to format and style basic elements of a website” (UK) to “A text equivalent for every non-text element shall be provided (e.g., via ‘alt,’ ‘longdesc,’ or in element content)” (USA). On the other hand, the heuristics also contained guidelines that are independent of technology. Examples of these include:</p>

   <p>“Provide forms with instructions for the visitor where necessary, in particular next to the input field where they matter” (NL),</p>
   <p>“For text, no color (combinations) are used that are difficult or illegible for (groups of) people” (NL), and</p>
   <p>“The site should preferably be multilingual and editing should be of high quality” (EU).</p>


   <p>The emphasis on technology-related items is understandable, because of the amount of technology required to keep the internet and every website running smoothly. However, websites should also be seen and treated as important communication media for organizations. In the fields of technical communication and document design, a large body of research is devoted to general principles of effective communication, helping readers to find and comprehend information. These principles hardly seem to be incorporated into these heuristics. Some guidelines refer to editing quality, such as “concise” or “clear, precise language” (UK), but such guidelines are rare. Considering that 76 percent of the guidelines have some relation to technology, it seems that the heuristics are more concerned with the medium as information carrier than with finding and comprehending information.</p>

   <p>Another way to look at the content of heuristics, is to analyze what information is covered by the heuristics (<a>Table 6</a>). Of the 434 product-focused items, 25 percent dealt with accessibility issues, whereas 43 percent dealt with usability. The largest part of the heuristics were thus aimed at providing an optimal user experience to as many users as possible. In addition, 24 percent of the guidelines were concerned with the correct application of programming languages such as HTML, CSS and scripting. Examples of these guidelines include:</p>

   <p>“Markup shall be used to associate data cells and header cells for data tables that have two or more logical levels of row or column headers” (USA), and</a>
   <p>“Markup languages (especially HTML) and stylesheets should be used according to their specifications and formal definitions” (Germany).</p>

   <p>The category, ‘Other,’ contained guidelines on a multitude of topics, ranging from a demand for interactivity and privacy concerns to text quality and product support documentation.</p>

   <p>The heuristics differed in the amount of attention they paid to usability and accessibility. The German and USA heuristics, which present themselves as accessibility-focused, contained relatively more accessibility items, while the British and Dutch heuristics contained relatively more usability guidelines. One could also conclude from the large amount of guidelines in the accessibility, usability, and programming language categories that the heuristics aim more strongly at a “correct” way of delivering the content than at helping the user with finding and comprehending the content. The few guidelines that echo general principles from technical communication and document design fell into the ‘Other’ category.</p>

   <h3>4.3. Validity of the heuristics</h3>

   <p>The e-Government heuristics gave hardly any indications about their foundations. The German BSI agency lists in its wider accessibility document several other sets of heuristics, such as the WCAG 1.0 heuristics, relating these to the BITV checklist. The Dutch heuristics contain a guideline that explicitly refers to the WCAG 1.0, to the effect that all government websites must comply with these guidelines. In fact, all five heuristics either state somewhere that the WCAG 1.0 level A guidelines must be followed, or list this set of guidelines as an important starting point for their heuristics. As a result, all five heuristics contain, at least in part, the same items, corresponding to the WCAG 1.0 (level A items). Because the WCAG 1.0 guidelines are an example of standards-based heuristics, one could conclude that the e-Government heuristics, or at least the accessibility parts in these documents, were all examples of standards-based heuristics. This was even more true for the Section 508 heuristics. In the preamble to Section 508, the Access Board acknowledges the WCAG 1.0 guidelines and lists the comments from the field solicited in the consultation round as important sources for this heuristic.</p>

   <p>It is interesting to note that the British, Dutch, and EU heuristics do not list sources for the guidelines on other topics. The authors of the British heuristics do mention a list of organizations they have consulted about the content, but they do not attribute specific guidelines to specific organizations. As for the Dutch heuristics, these were also developed as a cooperation between government agencies and commercial parties and the authors claim in the background information that “the Web guidelines are based on internationally recognized standards for sustainability and accessibility of websites.” However, these standards, except for the WCAG 1.0, were not listed. Following a rather lenient definition, these non-accessibility parts of the British and Dutch heuristics could be labeled as standards-based. Information about the basis for the EU heuristics was missing completely. The five heuristics were all, to a certain extent, developed in cooperation between government agencies, commercial parties and NGO's.</p>

   <h3>4.4. Presentation format of the heuristics</h3>
   <p>To describe the presentation format of the heuristics, we looked at the structure, the formulation of the items and the level of abstractness of the items. The BITV checklist and the EU Information Providers Guide seemed to have a random structure; the others were loosely clustered around topics or divided in sections. See for example the Dutch Web Guidelines (cf. Table 2) and the subparts of Section 508, where the technical standards, functional performance criteria and information, documentation, and support were listed under separate subparagraphs. The British guidelines took this even further by creating different checklists for different subjects, both per subsection and via the four other checklists. This clustering may aid designers or evaluators. If they need to know more about color use, it helps when all relevant advice is clustered in one section. One could argue that the German and EU heuristics are short enough, 14 and 10 items respectively, to facilitate memorization. However, because both of these heuristics contain a great number of “subitems” or subchecklists, these heuristics are in effect still difficult to memorize.</p>

   <p>Regarding the formulation of the items, requirements seemed a more popular format than instructions and questions (cf. <a>Table 7</a>). This preference for requirements might be related to the function of the heuristics. If heuristics have a verifying function, i.e. referring to requirements that websites need to meet, and their use is obligatory, then formulating their contents as requirements makes sense. The Dutch heuristics offer some support for this notion. Their official division in 30 requirements, 30 tests, and 124 guidelines is mirrorred in 45 requirements, 30 questions, and 109 instructions, with 15 of the guidelines also formulated as requirements. The items in the requirements-section are meant to be used at the development phase of a website, whereas the questions in the test-section are meant to check (afterwards) whether these requirements were met. Of the 139 questions, 64% required a closed answer, such as yes/no, and these seemed to correspond to the troubleshooting and verifying function. Another 32% of the questions required an open answer; these seemed to have an idea-generating function. Finally, 4% of the questions were a mix between open and closed.</p>

   <p>Regarding the level of items, 158 items (36%) could be described as high-level items; 276 items as low-level items (64%) (cf. <a>Table 8</a>). The item “Keep pages simple and easy to understand” (UK) was an example of the former, whereas the item “A text alternative must be offered when an imagemap is used” (UK) was considered a low-level item. The heuristics differed in the ratio of high and low-level items. Generally speaking, the larger the heuristics were, the more low-level items they contained. As mentioned earlier, high-level heuristics list the goal to be achieved, whereas low-level heuristics list the specific action to be executed. Low-level items can be used to expound the purpose of a high-level heuristic. In the British and Dutch heuristics there seemed to be room to include both high-level and low-level items. In the Dutch heuristics we found a strong preference for high-level items in the requirement- and test-section and more low-level items in the instruction-section. The European and German heuristics almost exclusively featured high-level items, such as “The EUROPA site must be accessible to the largest possible number of users” (EU). However, as referred to in section 2.3, both heuristics contained many embedded checklists or “prerequisites,” which gave more detailed advice on how to achieve the high-level aim. If these (large) subchecklists had been included in the analysis, the ratio of high- and low-level items in these heuristics would have resembled the ratio in other heuristics more closely. It seems that in these heuristics, the authors expect the users to need both the high- and low-level items.</p>

   <h2>5. Summary of findings</h2>
   <p>The purpose of this study was to analyze five current e-Government heuristics, and to explore the underlying assumptions about the way they function. We conclude that the government heuristics are very complex documents. It may be difficult for users to see the forest for the trees and know where to find relevant information within the documents.</p>

   <p>Our analyses (cf. <a>Table 9</a> for a summary) show that on the one hand, web designers and evaluators are obliged to use these heuristics, but on the other hand, the documents themselves seem difficult to use, given their size, the nesting of checklists, and the diverse content. Compliance with the heuristics may also be difficult to check. Verifying whether an e-Government website conforms with all the guidelines requires an extensive amount of automated and expert evaluation techniques, which is particularly difficult for the context- and process-focused guidelines.</p>

   <p>If we look at the presentation format of the various heuristics, we see very diverse structures, combinations of high- and low-level items, as well as various combinations of questions, instructions, and requirements. The presentation format is important, because of the complexity and size of these heuristics. Web designers and evaluators need to keep track of many different guidelines when working on a website.</p>

   <p>In section 3, we demonstrated that these heuristics are accompanied by much background information on how and why to apply them, but information about the foundations of the heuristics is scarce and validation research seems to be non-existent. This makes it very difficult for designers and evaluators to judge the quality of the heuristics. Are they based on recent research or on the intuitions of one or more practitioners, however valuable these insights may be? If based on research, how are the research findings translated in general advice?</p>

   <h2>6. Discussion</h2>


   <p>The five governments behind the heuristics all had the same problem, i.e. how to increase and/or ensure the quality of e-Government websites. They developed sets of heuristics as one means of quality control. Although many experts within the field of website evaluation think user-focused methods are the golden standard to test the quality of websites, one has to take into account the enormous amount of information and services available on e-Government websites. In light of the size of most e-Government websites, regular usability testing on all parts of the website does not seem feasible within time and financial constraints under which most organizations function. Expert evaluations, and heuristic evaluations in particular, might therefore be a good runner-up solution. Among others, the USA, UK, German, and Dutch governments have made it compulsory for their websites to comply with their respective heuristics.</p>

   <p>Based on our analyses, we conclude that these government heuristics are very complex documents. In a sense, these heuristics resemble other government instruments that are used to influence society, such as laws and regulations. These are aimed at achieving a certain goal, and, depending on the measurability of the goal, one can suffice with just stating the goal (goal-based rules) or one needs to define more specific procedures and concrete actions to perform (solution search rules and action-/state-based rules) (<a href="#8">Hale &amp; Swuste, 1998</a>). This is also illustrated by the heuristics in this study. The heuristics reflect a tension within the organizations between the controlability of the heuristics, leading to concrete and specific action/state-based guidelines, and less constrained guidance for quality optimization of the websites, leading to general, abstract goal-based rules.</p>

   <p><a href="#13">Mahler and Regan (2007)</a> describe how in U.S. government agencies tension and conflicts have arisen over the control of the website content. Different parties within these agencies, e.g. program officials, public affairs offices, IT divisions and legal counsel, all have an interest in the website and try to control it. On the one hand, this is an example of the different types of expertise that are all needed to create a good quality website. However, requirements from different fields of expertise can conflict with each other. Organizations need to find a way to reconcile these tensions. Something parallel can be seen within the content of the heuristics. Given the public character of the websites, it is important that these websites are accessible and usable by as many groups in society as possible. In addition, the relatively large amount of heuristics that are technology-related or list specific programming commands is understandable given the technical background of websites. However, websites are used by governments as a means of communication with the public (<a href="#29">West, 2008</a>). If we compare these heuristics for web communication to the literature on document design, such as brochures, one notices that the many research and practitioner guidelines for paper documents focus primarily on the content and not on the medium. In the e-Government heuristics, many guidelines aim to create the optimal carrier and relatively little attention is paid to user comprehension of the content . In this sense, the heuristics are aimed at two target groups, communication and content experts (i.e. public affairs offices and program managers) vs. web designers and IT experts (i.e. IT divisions). The latter group will probably find much of interest in the heuristics, but writers seem to be left with only a few and very abstract guidelines about editing quality. Translating these guidelines into usable content for a website requires more background knowledge and experience on the part of the expert.</p>

   <p>We have shown that little information is provided about foundations of the heuristics and validation research. We think that more information about these topics may benefit the authors of the heuristics, because designers might be more inclined to follow the advice if they see empirical evidence for their value (see, for instance, the “Research-based Web Design &amp; Usability Guidelines” by the <a href="#21">US Department of Health and Human Services (2006)</a>). It could make the heuristics more influential.</p>

   <p>This relates to another interesting phenomenon. Influential heuristics, such as WCAG 1.0, are, sometimes with small modifications and additions, incorporated in these governemnt heuristics. This raises the question as to how government guidelines should relate to international standards. Why do they not simply refer to these standards instead of incorporating them with modifications? Probably, most professional government bodies have agreed with these international standards. Two different versions of the same set of guidelines, with small modifications, do not improve the efficiency of government website evaluation processes.</p>

   <p>E-Government websites are complex communication products, often consisting of tens of thousands of pages. These websites need to be as simple to use as possible, and for all segments of the public. Heuristics are meant to aid in the accomplishment of this goal and many of the guidelines apply to all of the pages. But, the question remains: how can a government, or even an organization functioning as part of a national government, control and enforce the compliance of thousands of web pages with a simple set of criteria? The complexity of the government heuristics might make this task even more difficult. We do not yet know how experts integrate heuristics into their work, but a clear structure might aid them in this. We also do not know which impact factors such as item formulation and item level have on the (mental) workload of the expert.</p>

   <p>The fact that different levels and formulations are used, seems to indicate that they may be assumed to have different impacts. High-level heuristics with a limited set of general criteria may be unreliable, since experts must interpret abstract and open criteria, while low-level heuristics with many detailed criteria may be hard to manage in an evaluation process, and may hinder experts to monitor their evaluation activities. The level of the items might also impact the compliance checking process. The same kind of questions arise for the differences in formulation. The academic community has a task: to systematically study these differences and their impact on workprocess and the results of heuristic evaluations. In the meantime, if governments are serious about ensuring the quality of their sites, they should make designing and evaluating them as easy as possible for the web experts.</p>

   <p>Given the complexity of these documents, we seriously doubt whether these heuristics aid the experts in their work. In future research, we want to focus more on the work processes during heuristic evaluations and the effects of more or less complex heuristics on the results of heuristic evaluations. In one study, we will let experts perform a heuristic evaluation while thinking aloud, to gain insight into their thought processes/work processes. In a second study, our aim is to let municipal web communication experts work with heuristics of varying complexity and to study their results in terms of the quality of the evaluation (number and types of problems discovered) and their (self-reported) experiences with the evaluation process. Based on the results of this study, we conclude that web designers and evaluators certainly can find useful advice in the heuristics, but verification of compliance with these guidelines might be a hard job.</p>


   <h2>References</h2>

   <p id="1">Advies Overheid.nl, 2005</p>
   <p>Advies Overheid.nl. Webrichtlijnen voor de overheid (versie 1.2). (2005) Retrieved February 28, 2006, from http://webrichtlijnen.overheid.nl/</p>
   <br>

   <p id="2">Anderson et al., 2004</p>
   <p>S. Anderson, P.R. Bohman, O.K. Burmeister, G. Sampson-Wild. User needs and e-government accessibility: The future impact of WCAG 2.0. Lecture Notes in Computer Science, 3196 (2004), pp. 289–304</p>
   <br>

   <p id ="3">Bertot and Jaeger, 2008. J.C. Bertot, P.T. Jaeger. The e-government paradox: Better customer service doesn't necessarily cost less. Government Information Quarterly, 25 (2) (2008), pp. 149–154</p>
   <br>

   <p id="4">Bundesamt für Sicherheit in der Informationstechnik (BSI), 2005. Bundesamt für Sicherheit in der Informationstechnik (BSI). Barrierefreies e-government - leitfaden für entscheidungstraeger, grafiker und programmierer. E-Government-Handbuch(2005). Retrieved January 31, 2006, from https://www-bsi-bund-de.libproxy.lib.unc.edu/cae/servlet/contentblob/476832/publicationFile/28055/4_Barriere_pdf.pdf</p>
   <br>

   <p id="5">De Jong and Van der Geest, 2000</p>
   <p>M. De Jong, T. Van der Geest. Characterizing web heuristics Technical Communication, 47 (2000), pp. 311–326</p>
   <br>

   <p id="6">European Commission</p>
   <p>European Commission (n.d.). Information Providers Guide. Retrieved January 19, 2006, from http://europa.eu.int.libproxy.lib.unc.edu/comm/ipg/index_en.htm</p>
   <br>

   <p id="7">Evans, 2000. M.B. Evans. Challenges in developing research-based web design guidelines. IEEE Transactions on Professional Communication, 43 (2000), pp. 302–312</p>
   <br>

   <p id="8">Hale and Swuste, 1998</p>
   <p>A.R. Hale, P. Swuste. Safety rules: Procedural freedom or action constraint? Safety Science, 29 (3) (1998), pp. 163–177</p>
   <br>

   <p id="9">Hart and Teeter, 2003</p>
   <p>D.P. Hart, M.R. Teeter. The new e-government equation: Ease, engagement, privacy and protection. Council for Excellence in Government, Washington, DC (2003) Retrieved February 28, 2006, from http://www.cio.gov/Documents/egovpoll2003.pdf</p>
   <br>

   <p id="10">Holden et al., 2003</p>
   <p>S.H. Holden, D.F. Norris, P.D. Fletcher. Electronic government at the local level: Progress to date and future issues. Public Performance &amp; Management Review, 26 (2003), pp. 325–344</p>
   <br>

   <p id="11">Landis and Koch, 1977</p>
   <p>J.R. Landis, G.G. Koch. The measurement of observer agreement for categorical data. Biometrics, 33 (1977), pp. 159–174</p>
   <br>

   <p id="12">Leporini and Paternò, 2008</p>
   <p>B. Leporini, F. Paternò. Applying web usability criteria for vision-impaired users: Does it really improve task performance? International Journal of Human-Computer Interaction, 24 (2008), pp. 17–47</p>
   <br>

   <p id="13">Mahler and Regan, 2007</p>
   <p>J. Mahler, P.M. Regan. Crafting the message: Controlling content on agency web sites. Government Information Quarterly, 24 (2007), pp. 505–521</p>
   <br>

   <p id="14">Mayhew, 2005</p>
   <p>D.J. Mayhew. A design process for web usability. R.W. Proctor, K.-P.L. Vu (Eds.), Handbook of human factors in web design, Lawrence Erlbaum, Mahwah, NJ (2005), pp. 338–356</p>
   <br>

   <p id="15">Nielsen, 1994</p>
   <p>J. Nielsen. Heuristic evaluation. J. Nielsen, R.L. Mack (Eds.), Usability inspection methods, John Wiley, New York, NY (1994), pp. 25–62</p>
   <br>

   <p id="16">Office of the e-Envoy, 2003</p>
   <p>Office of the e-Envoy. Guidelines for UK government websites: Illustrated handbook for web management teams. Stationery Office (2003)</p>
   <br>

   <p id="17">Paddison and Englefield, 2004</p>
   <p>C. Paddison, P. Englefield. Applying heuristics to accessibility inspections. Interacting with Computers, 16 (2004), pp. 507–521</p>
   <br>

   <p id="18">Schriver, 1989</p>
   <p>K.A. Schriver. Evaluating text quality: The continuum from text-focused to reader-focused methods. IEEE Transactions on Professional Communication, 32 (1989), pp. 238–255</p>
   <br>

   <p id="19">Tang et al., 2006</p>
   <p>Z. Tang, T.R. Johnson, R.D. Tindall, J. Zhang. Applying heuristic evaluation to improve the usability of a telemedicine system. Telemedicine and e-Health, 12 (1) (2006), pp. 24–34</p>
   <br>

   <p id='20'>Uldall-Espersen et al., 2008</p>
   <p>T. Uldall-Espersen, E. Frokjaer, K. Hornbaek. Tracing impact in a usability improvement process. Interacting with Computers, 20 (1) (2008), pp. 48–63</p>
   <br>

   <p id='21'>US Department of Health and Human Services, 2006</p>
   <p>US Department of Health and Human Services. Research-based web design &amp; usability guidelines version 2. U.S. Government Printing Office, Washington, D.C (2006) Available at: http://www.usability.gov/pdfs/guidelines.html</p>
   <br>

   <p id="22">USA Architectural and Transportation Barriers Compliance Board, 2000</p>
   <p>USA Architectural and Transportation Barriers Compliance Board (Access Board). (2000). Electronic and information technology accessibility standards, Federal Register / Vol. 65, No. 246 / Thursday, December 21, 2000 / Rules and Regulations (Federal Register / Vol. 65, No. 246 / Thursday, December 21, 2000 / Rules and Regulations ed.). Available at: http://www.access-board.gov/sec508/508standards.pdf</p>
   <br>

   <p id="23">Van der Geest and Spyridakis, 2000</p>
   <p>T. Van der Geest, J.H. Spyridakis. Developing heuristics for web communication: An introduction to this special issue. Technical Communication, 47 (3) (2000), pp. 301–310</p>
   <br>

   <p id="24">Van der Geest and Spyridakis, 2000</p>
   <p>T. Van der Geest, J.H. Spyridakis. Developing heuristics for web communication: An introduction to this special issue. Technical Communication, 47 (3) (2000), pp. 301–310</p>
   <br>

   <p id="25">Van Deursen et al., 2006</p>
   <p>A. Van Deursen, J. Van Dijk, W. Ebbers. Why e-government usage lags behind: Explaining the gap between potential and actual usage of electronic public services in the Netherlands. Lecture Notes in Computer Science, 4084 (2006), pp. 269–280</p>
   <br>

   <p id="26">Van Dijk et al., 2007</p>
   <p>J. Van Dijk, W. Pieterson, A. Van Deursen, W. Ebbers. E-services for citizens: The Dutch usage case. Lecture Notes in Computer Science, 4656 (2007), pp. 155–166</p>
   <br>

   <p id="27">W3C, 1999</p>
   <p>W3C. (1999, 1999-5-5). Web content accessibility guidelines 1.0. Retrieved 2006-02-28, from http://www.w3.org/TR/WCAG10/.</p>
   <br>

   <p id="28">Welle Donker-Kuijer et al., 2008</p>
   <p>M. Welle Donker-Kuijer, M. De Jong, L. Lentz. Heuristic web site evaluation: Exploring the effects of guidelines on experts’ detection of usability problems. Technical Communication, 55 (2008), pp. 392–404</p>
   <br>

   <p id="29">West, 2008</p>
   <p>D.M. West. Improving technology utilization in electronic government around the world, 2008. Brookings Institution, Washington, D.C (2008) Available at: http://www.brookings.edu.libproxy.lib.unc.edu/~/media/Files/rc/reports/2008/0817_egovernment_west/0817_egovernment_west.pdf</p>
   <br>

   <h2>Vitae</h2>
   <p>Marieke Welle Donker-Kuijer is a PhD candidate at the University of Twente,Twente, The Netherlands. Her PhD research focuses on the methodology of evaluating informative Web sites, more specifically on the merits and drawbacks of heuristic expert evaluation.</p>

   <p>Menno de Jong is an associate professor of communication studies at the University of Twente, Twente, The Netherlands. His main research interest concerns the methodology of applied communication research.</p>

   <p>Leo Lentz is an associate professor of communication studies at Utrecht University, Utrecht, The Netherlands. Web site usability and document design are the main focuses of his research.</p>


   <!-- Paste the html for article 2 here -->
</body>