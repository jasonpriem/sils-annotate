<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" class="no-js no-jr">
<head>
   <title>Pilot Test</title>
   <meta charset="utf-8">

   <!-- custom stuff for sils-annotator -->
   <link rel="stylesheet" href="{{ url_for('static', filename='css/annotator.css') }}" />
   <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}" />

   <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
   <!--<script type="text/javascript" src="{{ url_for('static', filename='js/jquery-1.9.1.js') }}"></script>-->
   <script type="text/javascript" src="{{ url_for('static', filename='js/underscore.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/jquery.scrollTo-1.4.3.1-min.js') }}"></script>

   <script type="text/javascript"> apiRoot = "{{ g.api_root }}"</script>


   <!-- stuff for the off-the-shelf annotator.js -->

   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/preamble.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/class.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/console.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/extensions.js') }}"></script>


   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/range.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/annotator.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/plugin/store.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/plugin/scrollbar.js') }}"></script>


   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/widget.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/editor.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/viewer.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/annotator/notification.js') }}"></script>
   <script type="text/javascript" src="{{ url_for('static', filename='js/silsannotate.js') }}"></script>

</head>
<body>
   <h1>Pilot Test Article</h1>
   <h1>Altmetrics: Rethinking the Way We Measure</h1>
   <p>Finbar Galligan, Sharon Dyas-Correia</p>

   <h2>Abstract</h2>
   <p>Altmetrics is the focus for this edition of “Balance Point.” The column editor invited Finbar Galligan who has gained considerable knowledge of altmetrics to co-author the column. Altmetrics, their relationship to traditional metrics, their importance, uses, potential impacts, and possible future directions are examined. The authors conclude that altmetrics have an important future role to play and that they offer the potential to revolutionize the analysis of the value and impact of scholarly work.</p>

   <h2>1. Introduction</h2>
   <p>This installment of the “Balance Point” examines the relatively new area of metrics called “altmetrics.” When researching this topic, the column editor was struck by how much of the dialog around altmetrics is found in non-traditional places for academic discourse like blogs, wikis, Twitter, and various Web sites. It seemed fitting to ask someone actively involved in the dialog to participate in writing this column and therefore the column editor invited Finbar Galligan, who works for Swets Information Services and blogs about industry topics, to co-author the article. Galligan has written several thought-provoking blogs on altmetrics and related subjects and has developed considerable knowledge of the issues surrounding the new metrics.
   </p>

   <h2>2. What Are Altmetrics?</h2>
   <p>It is important to begin by defining altmetrics and what they do. Several characterizations are offered by numerous authors including the following:
   </p>
   <p>1. “Altmetrics—short for alternative metrics—aims to measure Web-driven scholarly interactions, such as how research is tweeted, blogged about, or bookmarked” <a href = "#test_1">(Howard, 2012)</a>.</p>
   <p>2. “Altmetrics are new measurements for the impact of scholarly content, based on how far and wide it travels through the social Web (like Twitter), social bookmarking (e.g. CiteULike) and collaboration tools (such as Mendeley) … What altmetrics hope to do is provide an alternative measure of impact, distinct from the Journal Impact Factor, which has been categorically misused and is unable to respond to the digital environment that scholarship takes place in today” <a href="#test_2">(Galligan, 2012 August 29)</a>.</p>
   <p>3. “Altmetrics specifically looks at the social Web and uses it to mine information for the analysis and detailed examination of scholarship” <a href="#test_3">(Altmetrics, n.d.)</a>.</p>
   <p>4. “Altmetrics go beyond traditional citation-based indicators as well as raw usage factors (such as downloads or click-through rates) in that they focus on readership, diffusion and reuse indicators that can be tracked via blogs, social media, peer production systems, collaborative annotation tools (including social bookmarking and reference management services)” <a href="#test_4">(Taraborelli, n.d.)</a>.</p>

   <p>Each of the above definitions is accurate although they vary in the amount of detail provided. However, it is clear from all of the definitions that altmetrics examine the content of the social Web in order to provide either an alternative or enhancement to the use of journal impact factors and click-through rate analysis to measure the impact and value of scholarly work.</p>

   <h2>3. What is the Relationship of Altmetrics to Traditional Metrics?</h2>

   <p>Proponents of altmetrics cite several reasons that necessitate the creation of new metrics. Peer review, citation counting, and journal impact factors have traditionally been used as a means of ascertaining the value of scholarly work and as a way of filtering out only the most significant and relevant material from the huge volume of academic literature produced. As the volume of material has increased and scholarly communication has moved online, the traditional metrics are failing <a href="#test_5">(Priem, Taraborelli, Groth &amp; Neylon, 2010)</a>. Traditional metrics have generally dealt with journals or articles and not measured other significant research output like blog posts, slideshows, datasets, and other important scholarly dialog. Altmetrics can measure impact at the journal article level as evidenced through social media activity as well as impact as measured by examining other significant research output. The new metrics offer the possibility to discover new insights into impact that have been previously impossible to obtain, and they are fast compared to traditional metrics that rely on citation counting and journal impact factors. Crowdsourcing means that an article's impact can almost immediately be assessed by multiple bookmarks and conversations. As pointed out in Almetrics: A Manifesto, “the speed of altmetrics presents the opportunity to create real-time recommendation and collaborative filtering systems” <a href="#test_6">(Priem et al., 2010)</a>.</p>

   <h2>4. What Altmetrics Studies Have Been Conducted?</h2>
   <p>The study of altmetrics is in its infancy but some work has been done. Researchers like Jason Priem, Dario Taraborelli, Paul Groth, and Cameron Neylon are responsible for writing Altmetrics: A Manifesto on the altmetrics.org Web site <a href="http://altmetrics.org">(http://altmetrics.org)</a>. Some empirical studies have been completed, but much work is yet to be accomplished. A comprehensive list of altmetrics articles about completed work and suggestions for future directions is available at the Mendeley group <a href="#test_4">(Taraborelli, n.d.)</a>. Additionally, Priem, Groth and Taraborelli have a recently published work that lists published articles related to altmetrics <a href="#test_7">(Priem, Groth &amps; Taraborelli, 2012)</a>.</p>

   <h2>5. What Altmetrics Tools and Software are Available?</h2>
   <p>With the growing interest in the use of altmetrics, several new tools have been created or are under development. The products listed below are some of the major new tools that have been developed over the last few years.</p>

   <h3>5.1. Plum Analytics</h3>
   <p>Plum Analytics <a href="http://www.plumanalytics.com/about.html">(http://www.plumanalytics.com/about.html)</a> collect data via open application programming interfaces (APIs) from various sources including blogs, Twitter, open access repositories that publish article level metrics (e.g., PLoS, Public Library of Science), data repositories, code source repositories (e.g., GitHub), scholarly social bookmarking sites (e.g., Mendeley, CiteULike), presentation sharing sites (e.g., SlideShare), grant funding data, link shortener metrics, among others <a href="test_8">(Kelley, 2012)</a>. CitedIn, ReaderMeter and ScienceCard are similar to Plum Analytics.

   <h3>5.2. CitedIn</h3>
   <p>The CitedIn Web site <a href="http://citedin.org/">(http://citedin.org/)</a> enables Web site users to track where they are cited using their PubMed Identifier. Citations in blogs, databases, and Wikipedia are among those included for analysis.</p>

   <h3>5.3. ReaderMeter</h3>
   <p>ReadMeter <a href="http://readermeter.org/">(http://readermeter.org/)</a> measures the use of scientific content by a large number of readers. It presents author and article level statistics visually. Data are obtained using the Mendeley API and the reports are available in both machine readable and HTML formats <a href="#test_9">(Priem et al., 2010)</a>.</p>

   <h3>5.4. ScienceCard</h3>
   <p>ScienceCard <a href="http://sciencecard.org/">(http://sciencecard.org/)</a> is not only a Web site but also provides application programming to collect article-level statistics for scientific articles. Users log in via Twitter and provide unique identifiers like Digital Object Identifiers (DOIs) or PubMed IDs. Data are currently collected from Twitter, Mendeley, PubMed Central, CiteULike, Wikipedia and CrossRef.</p>

   <h3>5.5. ImpactStory</h3>
   <p>ImpactStory <a href="http://impactstory.org/">(http://impactstory.org/)</a>, a re-launched version of Total-Impact, is a free open source Web application that collects data from a variety of open online sources to demonstrate the Web impact of a dataset, journal article or other research output that resides in a fixed online location. It then creates a single report. Data sources used by ImpactStory include Twitter, Mendeley, Delicious, CiteULike, F1000 reviews, SlideShare, GitHub, and a variety of other online sources <a href="#test_10">(Galligan, 2012, August 31)</a>.</p>

   <h3>5.6. Altmetric.com (Altmetric Explorer)</h3>
   <p>The altmetric explorer <a href="http://altmetric.com/">(http://altmetric.com/)</a> gathers data related to journal articles from news stories, blog posts, tweets, and postings related to scholarly articles and creates a score based on information from each of the data sources examined <a href="#test_11">(Galligan, 2012, August 31)</a>.</p>

   <h3>5.7. PLoS Impact Explorer</h3>
   <p>The PLoS Impact Explorer <a href="http://www.altmetric.com/demos/plos.html">(http://www.altmetric.com/demos/plos.html)</a> enables one to examine conversations that have been collected by altmetric.com that are related to papers published by the Public Library of Science (altmetrics.org). Information on tweets, facebook pages, re-editing articles, google posts, news mentions, and blogging activity is presented.</p>

   <h3>5.8. PaperCritic</h3>
   <p>PaperCritic <a href="http://www.papercritic.com/about">(http://www.papercritic.com/about)</a> enables researchers to monitor all types of feedback about their work and also enables everyone to evaluate the work of others. PaperCritic is powered by the Mendeley API.</p>

   <h2>6. What are the Issues Related to the Value of Various Altmetrics Tools?</h2>
   <p>It is difficult to isolate a single tool as each has its own niche value and focuses on specific groups in terms of data analyzed. For example, as outlined above, ReaderMeter explores readership; ScienceCard examines metrics at the researcher level; Altmetric.com investigates predominantly article-level metrics. There is no one-stop shop for everything because altmetrics are only beginning to be developed on a larger scale as well as to be accepted by a wider cross-section of the research community.</p>

   <p>Presenting data is one aspect, but making informed decisions is a separate issue. There is a lot of caution around treating altmetrics in a similar way to the impact factor, which has been applied in ways for which it was never designed. For example, basing the value of a single article on the impact factor of the journal in which it is published is a very poor application of the metric and without statistical meaning. The same is true for assigning impact factor values to a researcher. With altmetrics, there is a sense that the users themselves should articulate how the measurements should be applied to specific problems, rather than dictated by an organization that thinks it knows best. This is the main drawback of all altmetric tools—there is as yet no simple way to interpret the data and give clear meaning.</p>

   <h2>7. How are Altmetrics Possible Now?</h2>
   <p>Altmetrics have been made possible by the evolution of the Internet from a one-way Web page-visitor communication to the deeply integrated social structure it now represents. As more users interact with Web content, they leave more trails that are recorded; this is where the useful data relevant to altmetrics originates. By virtue of open APIs, open source code, and a widespread feeling of collaboration, it is possible to harvest these data and parse them into something readable and tangible.</p>

   <p>It is worth comparing altmetrics with traditional measurements of impact, particularly those that run on a closely-guarded system or algorithm, such as Thomson Reuters' Journal Citation Report <a href="http://thomsonreuters.com/products_services/science/science_products/a-z/journal_citation_reports/">(http://thomsonreuters.com/products_services/science/science_products/a-z/journal_citation_reports/)</a> and its resultant impact factor. As a for-profit enterprise, the company wants to protect its methods as the special recipe that adds value to its service. The final calculations for impact factors are largely unknown, and the underlying data are not subject to independent audit.</p>

   <p>An interesting editorial in the Journal of Cell Biology from 2007 outlines the main concerns with data transparency issues regarding the Thomson-Reuters impact factor <a href="#test_11">(Rossner, Van Epps &amp; Hill, 2007)</a>. This is the fundamental difference with altmetrics, which use mostly publically available data, making the process and calculations completely transparent. The other main difference between the two is time. Altmetrics happen instantly and are recorded, assimilated, and made available immediately. Journal–journal citations are much slower to be recorded and even slower still to be compiled into the impact factor score.</p>

   <p>The final instrumental factor in the rise and availability of altmetrics is the people involved. As we mention below, the entire idea of altmetrics was borne out of a host of frustrations with the legacy metrics handed down from generations of print publishing which no longer fit the online environment. We really owe the advent of altmetrics to the research and dedication of a group of individuals who seem poised to change how we think about research value and impact.</p>

   <h2>8. Why are Altmetrics of Interest to Libraries and Others?</h2>
   <p>Altmetrics are of interest to libraries and others for a variety of reasons. In the online environment, we track almost every movement: clicks, page views, and interactions as well as the way we share things with others. A great opportunity lies in the capture of the resulting data trail and building meaningful layers of insights onto it. Entire markets are built on data and interpreting its meaning, spotting trends and making informed decisions based on deep quantitative evidence—this has commonly come to be known as “big data.” Gartner is predicting 45% year-on-year increases in expenditure on analysis of social media, networks, and content, valuing the market at $45 billion dollars (US) by 2015 <a href="#test_12">(Columbus, 2012)</a>. This kind of analytical power should be relevant to many industries.</p>

   <p>Digital footprints suddenly become smaller parts of a greater picture, which is then open to intelligent analysis. Using publicly available data gathered from APIs from broad and sector-specific networks is something that cannot be ignored. Altmetrics present an integrated view of how one unit of content or one researcher has moved across the digital landscape in a series of actions or digital conversations. This can then be scaled up to clusters of articles, groups of researchers, or potentially even a combination of the two. This would allow departmental and organizational benchmarking, with a variety of different applications.</p>

   <p>If two specific roles of libraries are taken into account, then it becomes clear how altmetrics are a potentially valuable resource for them. Firstly, let's consider the library's role as a communications partner with researchers. When researchers are pressured to demonstrate the value they contribute, it is essential that research outcomes are disseminated effectively. In SwetsBlog, Priem emphasizes this very aspect of altmetrics and how altmetrics may help develop the changing role of the librarian, by developing their “increasingly important work in helping faculty understand and build their own impact” <a>(Galligan, 2012, August 31)</a>. By doing this, they may be able to show success of research programs against the overall institutional goals and what value they create for that institution. In addition, they will be able to actively measure that impact in real-time. The benefit for the librarian is a more integral position in the research process, moving towards assuming the role of what Priem calls, “scholarly communication specialists” <a>(Galligan, 2012, August 31)</a>.</p>

   <p>The second role of the library that we can consider is a learning support function. Creating the right learning environment and providing the right tools to students and faculty is one of the essential functions of the library. Providing primary and secondary scholarly content is a core aspect of this provision. However, collection development strategies tend to focus on the same indices that have been employed for many years—cost, usage and recommendations. Impact factor is also used widely as a tool for extra validation. This may have been enough in the days of freer budgets and perhaps less scrutiny. However, when library budgets are in decline or flat year-on-year (PCG Library Budget Predictions report for 2012 showed 0.2% increase in budgets in 2012 compared to a decrease predicted in 2011 <a>(Publishers Communications Group, 2012))</a>, more evidence is required to make truly informed decisions on the procurement and retention of academic content and to justify expenditures.</p>

   <p>It may be difficult to make sensible collection management decisions based on a single metric from a social network (e.g., number of tweets or blog posts relating to content from one journal), but there are new tools emerging that provide a different perspective on the traditional measures and a more balanced view from a wider dataset. One example of this is the new dashboard built around the Mendeley platform, Mendeley Institutional Edition. As the only current tool of its kind, it mines the enormous database of Mendeley's documents, user annotations and behavior to show simple and intuitive trends of usage, impact, and collaboration. Where does this help a library? By providing an altmetric overlay for journal usage that will complement the standard COUNTER (Counting Online Usage of NeTworked Electronic Resources, http://www.projectcounter.org/) statistics provided by publishers. It then goes deeper into the impact of the research output from faculty members and how the very content they are generating is being consumed by their peers, in real-time.</p>

   <p>Overall, it is the impact and value that altmetrics introduce within the library context, going beyond the traditional. The issue is finding an appropriate fit for altmetrics so they can fulfill their potential for specific circumstances. Altmetrics users themselves must determine how to employ particular altmetrics in order to deliver the correct scope and context. Thus, the reason altmetrics are important is that they constitute a very different companion to the traditional metrics of impact factor, h-index, eigenfactor, and so on, which are all citation-based metrics. Perhaps academia is not ready to surrender the traditional citation metrics yet, but when doubt is cast over the value of ‘alternative’ citations across a plethora of online networks and those citations are not just for journal articles (e.g., they would include datasets, presentations, images, graphics, etc.), we should really ask ourselves: why wouldn't we want to know this additional information? Once again, it is all related to impact but perhaps a type of impact with which the majority are not yet comfortable. It is very likely that researchers, and their evaluators alike, would want to know about new ways of proving that their contributions have value and impact <a>(Galligan, 2012, September 4)</a>.</p>

   <h2>9. Will Altmetrics Complement or Replace Traditional Metrics?</h2>
   <p>Initially, altmetrics will compliment traditional metrics, removing the complete reliance on the impact factor. They will provide wider and deeper insights into the impacts of scholarly articles, researchers, departments, universities, and so on—outside the reach of traditional metrics, which rarely exit the bounds of ‘traditional’ scholarship. This is where outreach again becomes a key driver. Many fields, but particularly the sciences, are increasingly pressured to demonstrate how research is relevant to the general public. It is important to combat an environment of suspicion and propaganda, painting researchers as working on ineffective projects funded by taxpayer money. Demonstrating relevance and utility is therefore essential to sustain a culture where research is encouraged and supported. When this relevance is measured and quantified, the system functions more efficiently and provides more evidence to support research work.</p>

  <p>In the long term, altmetrics may perhaps outlive the impact factor and other traditional citation-based metrics, but that would most likely be dependent on a substantial shift in the way scholarly output is presented in its most formal state. We are already seeing other scholarly output being promoted alongside the research article, facilitated by services that are mentioned elsewhere in this article, such as Figshare and other open repositories, GitHub (social code sharing repository), and more generic services like SlideShare for presentations.</p>

  <h2>10. Are Altmetrics About Value or Predicting What Will be Valued?</h2>
  <p>Altmetrics are about real-time impact and demonstrating that impact for a given research outcome. The impact in this context can be classed as value. Value becomes very important in the case of a university system placed under a systematic review, such as the REF (Research Evaluation Framework) in the United Kingdom. A key criterion for assessment in this example is the institution's “approach during the assessment period to enabling impact from its research, and case studies describing specific examples of impacts achieved during the assessment period of the evaluation” <a>(REF2014, 2012)</a>. Impact is left largely undefined in this case, which suggests some potential for altmetrics in the future.</p>

  <p>There is a growing wealth of early research looking at altmetrics as indicators of future value, including one correlating potential citations with tweets <a>(Eysenbach, 2011)</a>, and another comparing Mendeley stats (readership and sharing) with eventual citations based on a wide dataset of PLoS articles <a>(Priem, Piwowar &amp; Hemminger, 2012)</a>. The main feature of these points of reference is that they both refer back to the old metric of citation counting. There are some indications that content shared early after publication, which creates a lot of “online buzz,” may go on to become an influential paper. The inverse could also happen. For example, there is the case of the arsenic life paper by Wolfe-Simon in 2010 <a>(Wolfe-Simon et al., 2011)</a>, where a significant amount of online activity was generated around an article that was later proven to be inaccurate, and which probably went on to gain many citations based on being scientifically incorrect. Despite this, one could argue that the impact of this paper was to challenge the current scientific thinking behind origins of life, and although it was proven inaccurate, it still generated interest, discussion, and further research — all of which together could be classed as impact.</p>

  <h2>11. Are Altmetrics Relevant Just to Academics and Academic Libraries?</h2>
  <p>The idea of harnessing these metrics and using them together to create meaning began in the academic community with such innovators as Priem, Taraborelli, Groth and Neylon <a>(Priem et al., 2010)</a>. It was born out of the need to renovate scholarly communication by addressing the well documented problems with peer review, citation-based metrics, and the well-abused impact factor measurement <a>(Priem et al., 2010)</a>. Despite being born in the academic world, the concept of new measurements can also be relevant to other industries. Journalism is a good example of where impact is closely monitored to sell more papers or encourage more Web visits to a news service online.</p>

  <p>Let's examine a case that may appear divorced from the academic research community: a new product launch into the market from a commercial organization, manifested by a series of activities that constitute a campaign, led by a marketing team. From the outset, the marketing team has key tasks, objectives, and measures in place to monitor the success of each unit of activity (e.g., an online advertising initiative). It is very unlikely that this marketing team will focus on just one metric and declare, “Yes, we have succeeded.” Instead they will likely evaluate a variety of available and measurable metrics to define a formula that will indicate how well they succeeded against their initial goals—their key performance indicators. In the present digital environment, these will include organic sharing through social networks, on industry blogs, and perhaps even in databases specific to that industry or topic. In this sense, the marketing campaign can be substituted for the academic paper. Monitoring, evaluating, and reporting on success of that paper should be an integral part of the process—particularly since so many measurements are possible these days.</p>

  <h2>12. Are There Uses Beyond Academic Context?</h2>
  <p>As discussed above, units of academic research can now be assessed by a variety of metrics that help us understand their success in terms of impact. In the online environment for any industry, measuring the right things is of utmost importance. Research and development centers in corporations are evaluating the sum impact of large, semantically-linked sets of content and their real-time impact across the Web and social channels (including academic social channels like Mendeley) and can thus provide up-to-the minute snapshots of emerging research fronts and hot developments. For government, the aggregation of many data sources, including altmetrics, may provide better evidence to plan for investments, university funding, and return on investment (ROI) by having a complete picture of impact. Publishers, traditionally on the other side of the fence, can use altmetric signals to introduce new publishing programs based on current trends, not just the trends of two-year-old citation and impact factor data. Funding bodies, grant review panels, and other key decision making groups might use altmetrics as further evidence for decisions on grant awards, university tenure, and complete impact of a particular long or short term project.</p>

  <h2>13. What Will the Impact be on Traditional Peer Review?</h2>
  <p>The potential impact of altmetrics on peer review is an interesting topic. The system of peer review is one built on integrity and intra-disciplinary trust. There are arguments for a more open process where articles are “reviewed” by the community at large, as opposed to a select few with space in their schedules (finding peer reviewers is one of the most onerous tasks in the publishing process; getting reviewers to complete reviews quickly is even harder). Some models have already tried a new approach. One example is F1000 (http://f1000.com/prime). F1000 offers a post-publication review system comprised of many experts who will select research that meets certain conditions of excellence or novelty. However, the F1000 model relies on papers that are already published via traditional journal channels and have already undergone peer review already.</p>

  <p>Others have tried completely open publishing, but this approach lacks the obvious credibility and certification that academia requires; see Economics (an open electronic journal) for an example of open-source peer review <a>(http://www.economics-ejournal.org/)</a>. Scientific blogging is a third solution and fits with the altmetrics model (in terms of comments, shares, or mentions in other blogs). While credibility may be an issue with lesser-known authors, the rigorous structure of the academic journal article is mostly lost. Blogging is an excellent outreach endeavor, translating specific research for the general audience. Blogging is becoming more and more popular thanks to acceptance and encouragement from universities and research institutes. What is important to note is that impact can also be measured from content that is published on the Web, but which is not necessarily subject to a journal-style peer review. That is where altmetrics may be able to indicate quality and merit.</p>

  <h2>14. What is the Relationship of Altmetrics and Open Access Journals?</h2>
  <p>Altmetrics have always been closely associated with open access publishing: “Altmetrics' emphasis on openness aligns it with the open-access movement, whose goal is to make published research freely available online” <a>(Howard, 2012)</a>. While BMJ was the first to display article-level metrics on its site in 2004, JMIR: Journal of Medical Internet Research and PLoS were the first publications to look at other measures of impact at the article level, combining real-time (or near enough) usage data collected by article views, shares on common networks, and citations across a range of access and information providers <a>(Impact factor, 2012)</a>. PLoS has associated open access with altmetrics organically in some sense. Heather Piwowar (a co-founder of ImpactStory) discusses altmetrics in a 2011 blog post on “open impact tracking” <a>(Piwowar, 2011)</a> where open research and data have greater potential for impact, which will in turn be measured by the online indicators we have discussed in this article so far.</p>

  <p>Another way in which altmetrics are related to open access is that they are usually based (as indicated above) on open data or open source systems. A good example is Mendeley, which claims to have more citation records than Web of Science and Scopus. Mendely also boasts a great API, currently being used by 240 separate applications due to the vast amount of quality data they store. In a blog posting describing Mendeley that presents information on citation records and the Mendeley API mentioned above, Victor Henning (chief executive officer and co-founder of Mendeley) states, “Our vision was always to share this information with the academic community to make science more transparent” <a>(Henning, 2012)</a>.</p>

  <p>Open access does not solely relate to journal articles appearing in published journals. Over the past few years several new services have become available and gained real traction in the academic landscape. For example, institutional repositories are now widespread, with large amounts of university funding pouring into their development. Private enterprise is also catching up with respect to online storage as we see more and more services offering cloud storage solutions and hosting for content. University repositories pave a route to green open access, as well as a permanent, open storage for content generated by an institution and its faculty (slideshows, presentations, articles, data, etc.). While there are not always sophisticated or publically available metrics available for these, the opportunity is great, especially when referring back to the notion of impact and outreach. Even items stored in fixed Web locations within repositories are open to altmetrics reporting, as the majority of the items are freely accessible and indexed by the major search engines.</p>

  <p>There are also services like Figshare (http://figshare.com/) which is a startup funded by Macmillan's Digital Science incubator (http://international.macmillan.com/MediaArticle.aspx?id=2598), whose strapline is “get credit for all your research” (Thaney, 2010). Figshare is an open repository for all types of research output and has positioned itself in line with other services allowing resources to be fully citable, sharable and discoverable, all under a Creative Commons license.</p>

  <p>Crowdsourcing for peer review depends on openness and accessibility of the content to be reviewed, which is obviously intrinsic to the open access model. For crowdsourcing to be an effective way to offer an alternative method of peer review, it needs to confine itself to meaningful channels, such as Mendeley or a reputable blog network. By virtue of this, the act of mass peer review will create a matching set of analytic data that can then be analyzed as altmetrics, which in turn feeds back into the peer review process.</p>

  <p>Stephen Curry in a blog post in August 2012 (Curry, 2012) looks at the impact factor as one of the main culprits in the creation of a roadblock to open access. He argues that journals with high impact factors may maintain that higher author-pays fees are appropriate to publish in such quality publications (based around the notion that high rejection rates generate higher workloads). If the impact factor is banished, he insinuates, the valuation of scientific papers will be possible by the combined effort of the entire community, not by a single metric that has so many problems. This is where there is a possibility for altmetrics defined in the broadest sense here, as uncommon metrics for measuring the worth of a unit of academic content.</p>

  <h2>15. Can the use of Altmetrics Extend to E-books?</h2>
  <p>E-books are quite different from the other forms of content that have been discussed thus far, as they are generally much more substantial works containing many individual ideas. This immediately makes them difficult to compare to a single dataset or a journal article, which often focuses on a specific aspect of a wider research project. The simplest way to break a monograph down is into chapters for analysis, where the ideas will be much more consistent to one topic. Using the same measurements for e-books as we have examined for journal research may also not be appropriate. Further research would be needed to discover the most relevant channels through which books are discussed, reviewed and shared in the online environment. It is likely Twitter, blogs and other social networks would still play some part in a set of e-book metrics. Whether these measurements would mean the same to a book author as a journal author is still to be investigated. It is worth remembering that when we talk of altmetrics, we are discussing a very young field of study.</p>

  <h2>16. How Much Further Work is Needed to Make Altmetrics Truly Valuable?</h2>
  <p>Further work into altmetrics is clearly needed, and the increasing momentum over the past two years is testament to the keen interest that altmetrics have sparked. There are many aspects that will be explored, discussed, and researched, but it seems there are some fundamental steps that are required before we begin to see tangible and specific value from these metrics:</p>
  
  <p>Determining meaningful clusters of metrics for particular groups (e.g. libraries, publishers, research and development labs). As noted above, we have a vast array of data sources that can be wired together to give an overall picture. Customizing that picture for the right audiences is possibly the most valuable development that is yet to occur on a wider scale. For example, a research institute wants to know the overall impact they are having online, to benchmark themselves against a competing organization. They would need to carefully select those indicators that are most relevant to their work but also can be measured consistently for both parties. Streamlining this process into an intuitive dashboard is the second phase to making the altmetric data accessible for better interpretation.</p>

  <p>Avoiding the mistakes made with the impact factor. One of the original ideas for researching the value of altmetrics was to provide an accompaniment and eventually perhaps an alternative to the journal impact factor. It will no doubt remain as such in the future. The tendency to desire one single score to evaluate research is one of laziness. While a single score like the impact factor is certainly convenient, plenty of evidence suggests that its value is in fact rather limited and flawed for most applications. Moving forward with altmetrics must be done with caution to make sure that there is always a deeper relevance behind its surface results.</p>
  
  <p>Realizing altmetrics are young. The capabilities of the Web were not sophisticated enough to support almetrics until perhaps the last five years. Further research might well include longitudinal studies on direct correlations between altmetric indicators and, in the case of the sciences, increased collaboration between researchers and faster innovation as a result. Longer, more systematic studies on the importance of producing impact across Web channels will undoubtedly lead to new ways for researchers and universities to disseminate and actively promote their output.</p>
 
  <h2>17. Are There Other Issues and Important Related Ideas?</h2>
  <p>Gaming is an issue that comes up regularly when discussing traditional metrics and gaming is also one of the issues that appear regularly in discussions of the negative side of altmetrics. Michael Kelley suggests, however, that many think that the volume of data available may serve to inhibit gaming because patterns are easily detected in big data <a>(Kelley, 2012)</a>.</p>

  <p>Some critics comment that a high percentage of academics still use EndNote Web and RefWorks, as opposed to Mendeley and Zotero, and therefore the accuracy of the results would naturally be skewed because their references would not be analyzed <a>(ImpactStory, n.d.)</a>. Others suggest that in the case of Mendeley, statistics can be normalized for the relative number of people on Mendeley for given fields against the total population working in those fields.</p>

  <p>In addition to not using particular collaboration and bibliographic tools, critics suggest that many academics do not use social media at all so accuracy of data could again potentially be skewed (ImpactStory, n.d.). Others argue that the representative samples will probably be big enough to make some conclusions. Also, as we move forward, more academics will begin to participate in social media activities, especially when the next generation, predominantly digital-natives, becomes pre-eminent in their chosen fields.</p>
 
  <p>Critics point out that there is a need to be able to determine the value of the activities that are being analyzed to determine the quality of the interaction. Some question the value of blog posts, tweets, and mentions on social media as indicators of importance and scholarly impact and suggest that the “buzz” created may be just “buzz” (Priem et al., 2010, October, 26 and ImpactStory, n.d). Others suggest that employing a PageRank-style algorithm to blogs to determine the value of the citing blog source is a possible control for establishing a value for the dialog.</p>

  <h2>18. Conclusion</h2>
  <p>Many are beginning to view altmetrics as a bright new area in the field of metrics with the potential to revolutionize the analysis of the value and impact of scholarly work. It seems appropriate for the innovators in altmetrics to proceed rapidly, but with caution. Within the community there is some apprehension, such as that displayed by David Colquhoun, a professor at University College London, who has suggested that “the people who propose things like altmetrics are kids playing with computers” <a>(Jump, 2012)</a>, but most see altmetrics as having incredible potential. They view those working in the field as pioneers developing important tools for the future. Only time will tell what role altmetrics will eventually play in the analysis of scholarly work and of the impacts of scholars pursuing careers in academic fields, but it is clear that altmetrics will play a role, and many think it is likely to be an extremely important one.</p>

  <p id= "test">Altmetrics (n.d.).<br> HLWIKI International. Retrieved from <a href="http://hlwiki.slais.ubc.ca/index.php/Altmetrics">http://hlwiki.slais.ubc.ca/index.php/Altmetrics.</a></p>
  <p id="">Columbus, L. (October 15).<br> Using search analytics to see into Gartner's $232B big data forecast. Forbes (Retrieved from <a>http://www.forbes.com/sites/louiscolumbus/2012/10/15/using-search-analytics-to-see-into-gartners-232b-big-data-forecast/</a>)</p>
  <p id ="">Curry, St. (August 13).<br> Sick of impact factors. [Blog post]. Retrieved from <a>http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/</a></p>
  <p id="">Eysenbach, G. (2011). Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact. JMIR: Journal of Medical Internet Research, 13(4), e123, <a>http://dx.doi.org/10.2196/jmir.2012</a> (Retrieved from <a>http://www.jmir.org/2011/4/e123/</a>)</p>
  <p id ="">Galligan, F. (2012a). <br>Altmetrics for librarians and institutions: Part I. [Blog post]. Swets blog. Retrieved from <a>http://www.swets.com/blog/altmetrics-for-librarians-andinstitutions-part-i#.UJAmnVmhkyI </a></p>
  <p>Galligan, F. (2012b).<br> Librarians and the benefits of altmetrics: Part 2. [Blog post]. Swets blog. Retrieved from <a>http://www.swets.com/blog/altmetrics-for-librarians-andinstitutions-part-ii#.UJAmwFmhkyI</a></p>
  <p>Galligan, F. (2012c).<br> Altmetrics in thewider academic context: Part 3. [Blog post]. Swets blog. Retrieved from <a>http://www.swets.com/blog/altmetrics-for-librarians-and-institutionspart-iii#.UJAm41mhkyI</a></p>
  <p>Henning, V. (August 22).<br> Mendeley generating 100 m API calls from apps every month.
  [Blog post]. APPs Blog. Retrieved from <a>http://www.guardian.co.uk/technology/appsblog/2012/aug/22/mendeley-apps-api-growth</a></p>
  <p>Howard, J. (January 29).<br> Scholars seek better ways to track impact online. The Chronicle of Higher Education (Retrieved <a>from http://chronicle.com/article/As-Scholarship-Goes-Digital/130482/?sid=wc&amp;utm_source=wc&amp;utm_medium=en</a>)</p>
  <p>Impact factor (2012).<br> Wikipedia: The free encyclopedia. Retrieved from <a>http://en.wikipedia.org/wiki/Impact_factorImpactStory</a> (n.d.). HLWIKI International. Retrieved from <a>http://hlwiki.slais.ubc.ca/index.php/ImpactStory</a>.</p>
  <p>Jump, P. (August 23).<br> Research intelligence–alt-metrics: Fairer faster impact data. The Times Higher Education, 23, (Retrieved from <a>http://www.timeshighereducation.co.uk/story.asp?storycode=420926</a>)</p>
  <p>Kelley, M. (May 31). <br>Two architects of library discovery tools launch an altmetrics venture.The Digital Shift/Library Journal (Retrieved from <a>http://www.thedigitalshift.com/2012/05/social-media/two-architects-of-library-discovery-tools-launch-naltmetrics-venture/</a>)</p>
  <p>Piwowar, H. (October 31)<br>. The promise of another open: Open impact tracking.[Blog post]. Retrieved from <a>http://researchremix.wordpress.com/2011/10/31/open-impact-tracking/</a></p>
  <p>Priem, J., Groth, P., &amps; Taraborelli, D. (2012a).<br> The altmetrics collection. PLoS ONE, 7(11),e48753, <a>http://dx.doi.org/10.1371/journal.pone.0048753 (Retrieved from http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0048753</a>)</p>
  <p>Priem, J., Piwowar, H. A., &amps; Hemminger, B. M. (2012b).<br> Altmetrics in the wild: Using social media to explore scholarly impact. arXiv:1203.474yl (Retrieved from <a>http://arxiv.org/html/1203.4745</a>)</p>
  <p>Priem, J., Taraborelli, D., Groth, P., &amps; Neylon, C. (26).<br> Altmetrics: A Manifesto. Retrieved from http://altmetrics.org</p> 
  <p>Publishers Communications Group (2012). Library budget predictions for 2012. Retrieved from http://www.pcgplus.com/pdfs/Library%20Budget%20Predictions%20for%202012.pdf</p>
  <p>REF2014: Panel criteria and working methods.<br> Retrieved from http://www.ref.ac.uk/media/ref/content/pub/panelcriteriaandworkingmethods/01_12.pdf.(2012, January).</p>
  <p>Rossner, M., Van Epps, H., &amps; Hill, E. (2007)<br>. Show me the data. JCB, 179(6), 1091–1092,http://dx.doi.org/10.1083/jcb.200711140 (Retrieved from http://jcb.rupress.org/content/179/6/1091.full)</p>
  <p>Taraborelli, D. (n.d.). Mendeley. Group. Retrieved at http://www.mendeley.com/groups/586171/altmetrics/.</p>
  <p>Thaney, K. (December 7).Macmillan announces launch of Digital Science. [Press release].Retrieved from http://international.macmillan.com/MediaArticle.aspx?id=2598</p>
  <p>Wolfe-Simon, F., et al. (2011). <br> A bacteriumthat can grow by using arsenic instead of phosphorus.Science, 332(6034), 1163–1166, http://dx.doi.org/10.1126/science.1197258.</p>

</body>